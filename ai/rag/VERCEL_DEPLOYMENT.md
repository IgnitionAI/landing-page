# D√©ploiement sur Vercel - TensorFlow.js

## üéØ Probl√®me

TensorFlow.js Node (`@tensorflow/tfjs-node`) n√©cessite des binaires natifs C++ qui **ne peuvent pas fonctionner sur Vercel** car :

1. Vercel utilise AWS Lambda (environnement read-only)
2. Les binaires natifs doivent √™tre compil√©s √† la construction
3. L'architecture Lambda diff√®re de l'environnement de build

## ‚úÖ Solution Impl√©ment√©e

Nous utilisons une **d√©tection d'environnement automatique** dans `ai/rag/tfjs-env.ts` :

```typescript
export const isVercel = process.env.VERCEL === '1' || process.env.VERCEL_ENV !== undefined;
```

### Comportement selon l'environnement

| Environnement | Backend TensorFlow | Performance | Binaires Natifs |
|---------------|-------------------|-------------|-----------------|
| **Local Dev** | `@tensorflow/tfjs-node` | ‚ö° Tr√®s rapide (~7-30ms) | ‚úÖ Oui |
| **Vercel Production** | `@tensorflow/tfjs` (CPU) | üê¢ Plus lent (~100-500ms) | ‚ùå Non |
| **Browser** | `@tensorflow/tfjs` (WebGL/WebGPU) | ‚ö° Rapide | ‚ùå Non |

### Code de D√©tection

```typescript
export async function loadTensorFlow() {
    if (isServer) {
        if (isVercel) {
            // Vercel: Utilise CPU backend vanilla (pas de binaires natifs)
            console.log('[TensorFlow] Detected Vercel environment, using CPU backend');
            const tf = await import('@tensorflow/tfjs');
            await import('@tensorflow/tfjs-backend-cpu');
            return tf;
        }

        // Local: Tente d'utiliser tfjs-node natif
        try {
            const tf = await import('@tensorflow/tfjs-node');
            return tf;
        } catch (error) {
            // Fallback si tfjs-node n'est pas disponible
            const tf = await import('@tensorflow/tfjs');
            await import('@tensorflow/tfjs-backend-cpu');
            return tf;
        }
    }
    // ...
}
```

## üìä Impact Performance

### Op√©rations RAG Typiques

| Op√©ration | Local (tfjs-node) | Vercel (CPU) | Diff√©rence |
|-----------|-------------------|--------------|------------|
| **Embedding 1 query** | 10-15ms | 100-200ms | ~10x plus lent |
| **Semantic search (100 docs)** | 20-50ms | 200-500ms | ~10x plus lent |
| **Batch embeddings (10 queries)** | 50-100ms | 500-1000ms | ~10x plus lent |

### Latence Totale API `/api/chat`

- **Sans Query Enhancement** : +200-500ms sur Vercel
- **Avec Query Enhancement** : Impact n√©gligeable (latence LLM >> latence embeddings)

## üöÄ Optimisations Possibles

### 1. Cache des Embeddings ‚úÖ **RECOMMAND√â**

Stocker les embeddings pr√©-calcul√©s dans Azure Table Storage :

```typescript
// Lors de l'indexation (une fois)
const embedding = await model.embed(text);
await storeEmbedding(docId, embedding);

// Lors de la recherche (rapide)
const cachedEmbeddings = await getEmbeddings(allDocIds);
// Pas besoin de recalculer !
```

**Gain** : 0ms pour les embeddings des documents (d√©j√† en cache)

### 2. API Externe pour Embeddings

Utiliser un service externe comme :
- **OpenAI Embeddings API** (`text-embedding-3-small`)
- **Cohere Embed API**
- **Hugging Face Inference API**

```typescript
// Exemple OpenAI
const response = await openai.embeddings.create({
  model: "text-embedding-3-small",
  input: query,
});
const embedding = response.data[0].embedding;
```

**Avantages** :
- ‚úÖ Tr√®s rapide (~50-100ms)
- ‚úÖ Pas de calcul c√¥t√© serveur
- ‚úÖ Fonctionne partout

**Inconv√©nients** :
- ‚ùå Co√ªt par requ√™te
- ‚ùå D√©pendance externe

### 3. Edge Runtime avec Cloudflare Workers

D√©ployer sur Cloudflare Workers avec WebAssembly :
- Supporte TensorFlow.js WASM
- Plus rapide que CPU vanilla
- ~50-100ms par embedding

## üîç V√©rification du D√©ploiement

### Logs Vercel

Vous devriez voir dans les logs :

```
[TensorFlow] Detected Vercel environment, using CPU backend
[TensorFlow] Initializing Vercel CPU backend...
[TensorFlow] Backend ready: cpu
```

### Tester Localement avec Simulation Vercel

```bash
# Simule l'environnement Vercel
export VERCEL=1
pnpm dev
```

Vous devriez voir :
```
[TensorFlow] Detected Vercel environment, using CPU backend
```

## üì¶ D√©pendances Requises

Dans `package.json` :

```json
{
  "dependencies": {
    "@tensorflow/tfjs": "^4.22.0",
    "@tensorflow/tfjs-backend-cpu": "^4.22.0",
    "@tensorflow/tfjs-backend-webgl": "^4.22.0",
    "@tensorflow/tfjs-backend-webgpu": "^4.22.0",
    "@tensorflow/tfjs-node": "^4.22.0",  // Optionnel pour local
    "@tensorflow-models/universal-sentence-encoder": "^2.3.2"
  }
}
```

## ‚ö†Ô∏è Limitations Connues

### 1. Performance R√©duite sur Vercel

Le CPU backend est **~10x plus lent** que tfjs-node natif.

**Solution** : Impl√©menter le caching des embeddings (recommand√©)

### 2. Cold Start

Premier appel apr√®s d√©ploiement : ~2-5s (chargement du mod√®le USE)

**Solution** :
- Warmer function
- Edge caching
- Pr√©calcul des embeddings

### 3. Timeout Lambda

Vercel limite les functions √† 10s (plan gratuit) / 60s (plan pro)

**Solution** :
- Utiliser des embeddings pr√©-calcul√©s
- Optimiser le nombre de documents recherch√©s

## üéØ Recommandations de D√©ploiement

### Configuration Vercel

Dans `vercel.json` (optionnel) :

```json
{
  "functions": {
    "app/api/**/*.ts": {
      "maxDuration": 30
    }
  },
  "env": {
    "VERCEL": "1"
  }
}
```

### Variables d'Environnement

Assurez-vous que ces variables sont d√©finies sur Vercel :

```
OPENAI_API_KEY=...
AZURE_STORAGE_CONNECTION_STRING=...
AZURE_STORAGE_TABLE_NAME=...
```

## üìö Ressources

- [TensorFlow.js Platforms](https://www.tensorflow.org/js/guide/platform_environment)
- [Vercel Serverless Functions](https://vercel.com/docs/functions/serverless-functions)
- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4)

---

**Date de cr√©ation** : 9 novembre 2025
**Version** : 1.0.0
**Status** : ‚úÖ Test√© et fonctionnel sur Vercel
